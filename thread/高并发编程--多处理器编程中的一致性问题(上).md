# 高并发编程--多处理器编程中的一致性问题(上)
"Multithreading is just one damn thing after, before, or simultaneous with another. "[2]
## 0 写在前面
在分布式存储系统中，为了提高可靠性，通常会引入多个副本，多个副本需要向用户表现出一致的内容。这很自然的让人想到如何确保多副本之间的一致性。为此也有了paxos和raft等保证多副本之间一致性的协议。当我们在一个多处理器机器上编程时我们通常会忽略在多处理器环境下的一致性问题，因为系统已经为我们做好了基本的一致性保证，当存在一致性问题的时候上层编程语言也提供了具备一致性语意的接口，只是我们在编程中并没有意识到这些接口与一致性的关系。无论是分布式存储还是多处理器编程都有一个共同点，就是会涉及共享对象的操作。

一旦出现共享，就会出现正确性的问题，那么如何定义在并发中操作共享对象的正确性，这就是一致性协议的任务了。

本文主要针对多处理器系统中的一致性问题进行了一些总结，对于分布式中的一致性问题会在后面文章中总结。

多处理器中的一致性问题源于并发，源于共享。对于共享内存并发操作下的正确性保证是硬件设计者需要提供给上层开发人员最重要的保证。对于上层开发人员来说，系统内部的一致性是透明的，但是去了解系统内部一致性的设计及原理有利于我们更能够面向机器编程，写出正确的代码。

由于水平有限，文中如存在问题，请帮忙提出。
## 1 本文主要内容
本文主要包括以下内容：
- cache coherence
   - cache design
   - cache coherence protocol
   - store buffer & invalidate message queue
   - memory barrier/ fence

- memory consistency
   - consistency 分类定义
   - SC
   - x86 TSO
   - relaxed memory model

- C++ memory order
   - memory model and memory order
   - memory order release/acquire
   - memory order sequence
   - memory order relaxed

- Synchronize
   - insight lock

## 2 cache coherence[4]
一致性的任务就是保证并发操作共享内存的正确性，在讨论内存操作正确性的时候，通常分为两个部分：memory consistency 和 cache coherence[3]。

本节着重讨论cache coherence的实现。

### 2.1 cache vs buffer
在底层系统设计中通常会有cache 及 buffer的设计，两者都有缓存的意思，那么真正的区别在哪里呢，知乎上有个回答我感觉挺好的。[Cache 和 Buffer 都是缓存，主要区别是什么？](https://www.zhihu.com/question/26190832)
>Buffer（缓冲区）是系统两端处理速度平衡（从长时间尺度上看）时使用的。它的引入是为了减小短期内突发I/O的影响，起到流量整形的作用。比如生产者——消费者问题，他们产生和消耗资源的速度大体接近，加一个buffer可以抵消掉资源刚产生/消耗时的突然变化。\
Cache（缓存）则是系统两端处理速度不匹配时的一种折衷策略。因为CPU和memory之间的速度差异越来越大，所以人们充分利用数据的局部性（locality）特征，通过使用存储系统分级（memory hierarchy）的策略来减小这种差异带来的影响。
### 2.2 cache设计
从cache vs buffer的区别中我们可看出，cache的出现其实就是为了缓解CPU与主存之间的速度极度不平衡。因此通常CPU设计中引入了cache，分为L1 cache，L2 cache和L3 cache，其性能指数大致如下[11]：\
<img src="https://github.com/ashenone0917/image/blob/main/v2-ebc51ace0133b0e0794e0b50fc8bdd92_720w.jpg" width = "443" height = "495" alt="cpu cache" align=center />

通常L1 cache又会分为I cache和D cache，分别存放指令与数据，L1 cache通常是core独占的，L2可以是两个core共享，L3 cache则是所有core共享。
#### 2.2.0 CPU cache架构
如下图[4]， 目前的CPU架构中的cache通常架构如此，这里的cache是L1 cache。我们这里讨论的cache基本都是基于L1 cache的，因为L1 cache通常是core独占的，所以L1 cache的设计及数据一致性是难点也是重点。
![cache](https://github.com/ashenone0917/image/blob/main/v2-747891c1407aa113f3fed33f7a86527c_720w.jpg "")\
cache中的数据存放单位是cache line，通常一个cache line长度大概在16-256Bytes之间。CPU访问memory的时候首先会先查看cache中是否已经缓存该address，如果已缓存当前address并且是可用的，那么CPU就会直接从cache读取该值，这样极大的提高performance。如果所要访问的address不在cache中，CPU则要从memory读取，CPU会读取cache line大小的数据，并将其放入cache中这样下次读取的时候可以直接从cache中读。L1 cache通常在32KB大小，当cache被填满之后就需要按照一定策略将元素踢出去，这个策略可以是LRU或者随机等等。
这里有两个小点需要注意：
- L1 cache独占导致的数据不一致\
因为每个core都有自己独立的L1 cache，对于一个共享的memory location两个cache可以有自己的copy。那么这就会出现了数据不一致的状况，两个core可能同时访问这个memory location，且如果一个core是对这个memory location进行修改，那么这就需要两边的cache进行同步，防止数据不一致。这个工作就是cache coherence protocol要做的事情。
- cacheline导致的false sharing
```cpp
struct test {
  uint32_t a;
  uint32_t b;
 }
 test t;
 ```
 t是一个共享变量，core1上线程t1访问t.a， core2线程t2访问t.b,如果cacheline大小是16Bytes，那么当t1访问t.a时cache miss，然后会将这个t.a读取到cache中，但是为了提高效率，core每次读取的长度是cacheline的长度，t.a长度是4， t.b长度是4，所以在一次读取中会将t.a和t.b一次性读到了自己cache中了，那么t2也同样存在这个问题，如果t1修改当前cacheline中的值，他需要与core2对应的cacheline同步，那么本来两个可以并发的memory location引入了不必要的同步。对性能产生不必要的影响。\
 那么为了避免false sharing，就需要让这两个memory location进行cacheline对齐。C++ 提供了alignas的方法。
### 2.3 cache coherence protocol
前面小节提到了为什么需要coherence protocol，那么coherence protocol是如何保证cache之间的数据一致性的呢？

这里就需要引入MESI协议了，MESI是一种维护cacheline状态一致性的协议。MESI: 由Modified,Exclusive,Share,Invalid四种状态组合。
#### 2.3.1 cacheline state
- Modified(M):\
当一个core 的cacheline的状态是M时，说明当前core最近修改了这个cache，那么其他core的cache不能再修改当前cache line对应的memory location，除非该cache将这个修改同步到了memory。这个core对这个memory location可以理解为Owned。
- Exclusive(E):\
E这个状态与M很像，区别在于当前core并没有修改当前的cacheline，这意味着当前cacheline存储的memory location的值是最新的。当前core可以对该cacheline进行modify且不需要与其他core的cache同步。这个core对这个memory location可以理解为Owned。
- Share(S):\
S表示当前cacheline在其他core的cache也存在copy，当前core如果需要修改该cacheline则需要与其他core的cache进行提前沟通。
- Invalid(I):\
I表示当前cacheline是空的。

cacheline的状态变化需要在各个core之间同步，那么如何进行同步呢，MESI使用protocol message。
#### 2.3.2 protocol message
作为core之间的沟通工具，protocol message分为以下几种消息类型：
- Read\
当一个cache需要读取某个cacheline消息的时候就会发起read消息。
- Read Response\
read response是read的回应，这response可以来自其他core的cache也可以来自memory。当其他core中对当前cacheline是M状态时，则会发起read response。
- Invalidate\
Invalidate消息包含对应的memory location，接收到这个消息的cache需要将自己cacheline内容剔除，并响应。
- Invalidate acknowledge\
接收到Invalidate后删除cacheline中的数据就向发起者回复invalidate ack。
- Read Invalidate\
这个消息包含两个操作，read和invalidate，那么它也需要接收read response和多个invalidate ack响应。
- write back
writeback包含数据和地址，会将这个地址对应的数据刷到内存中。
### 2.4 store buffer and invalidate queue
MESI中的四种状态可以互相转化，具体状态迁移条件请参考[4]，这里不再赘述。
#### 2.4.1 store buffer
CPU设计者都是在极致压榨其性能。cache就是CPU设计者压榨CPU性能的一种体现，但是这样他们觉得还不够。考虑一种状况，假设当core1执行一次store操作，且这个store操作的memory location对应的cacheline在另外一个core2上是owned状态，那么core1需要向core2发送Invalidate message，并且需要等到core2返回invalidate ack之后才能继续向下执行，那么在这个期间core1就处于盲等阶段，那么core1必须要等这么久吗？于是CPU设计者引入了store buffer，这个buffer处于CPU与cache之间。如下图[4].
![](https://github.com/ashenone0917/image/blob/main/v2-3c314d1fc195ec2aeafb96e01f3babca_720w.jpg)

有了store buffer之后core1如果执行store操作就不用立刻向core2发送invalidate message了，core1只需要将store值添加到store buffer中即可。但是引入store buffer会带来两个问题。
- 考虑以下场景：
```cpp
  // a, b init to 0.
  a=1;
  b = a + 1;
  assert(b == 2);
 ```
 上述代码中，core执行a=1后，a=1这个值被放到core的storebuffer里了，然后继续执行b=a+1,这时候core的cacheline中保存的a还是原来的0.这个时候就会导致assert失败。因为我们在storebuffer里和cacheline中的a是两个独立的拷贝，所以导致这种错误的结果，因此CPU设计者通过使用"store Forwarding"的方式解决这个问题，就是在执行load操作时先去storebuffer中查找对应的memory location，如果查到就使用storebuffer中的最新值。
 - 考虑另外一个场景
```cpp
  void foo(void) {
      a=1;
      b=1;
  }
  
  void bar(void) {
      while (b == 0) continue;
      assert(a == 1);
  }
  ```
  
foo和bar两个函数如果在两个不同的core上执行，假设core1执行foo，core2执行bar，a不在core1的cache中，b在core1的cache中，且b对应的cacheline状态是M。

那么core1执行a=1的时候会将a=1放到storebuffer中，然后再执行b=1，因为b在core1上是M状态，所以修改b不需要与其他core进行同步，b的修改直接就在cacheline中进行了，所以也不会进storebuffer。这时候core2执行while(b==0)判断的时候发现b=1了，那么就会进入到assert，但这个时候如果a=1的storebuffer还没有更新core1中的a的cacheline的话，core2获得的a的值为0，那么这个时候结果也是不符合预期的。

但是在CPU设计层面是无法判断当前core中执行的变量是否与其他的core中的变量存在关系，因为CPU在执行代码的时候他认为这个当前所执行程序就是一个单线程的，他无法感知多线程的存在。因此这个问题无法在CPU设计层面解决，这个就需要编码人员介入了，编码人员需要告诉CPU现在需要将storebuffer flush到cache里，于是CPU设计者提供了叫memory barrier的工具。
